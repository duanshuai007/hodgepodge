#人工神经网络 ANN(Artificial Neutral Network)

神经网络实际上是一种分类的算法，是根据训练集中数据对象的属性和类别得到一种分类模型，让它可以根据属性对新的数据对象进行分类。神经网络更像是构建一个最优化的数学表达式。


## 1 神经元结构  

神经元具有多个输入和一个输入，每个输入都具有相同或不同的权重，输入分为兴奋性输入和抑制性输入，当神经元的输入达到阈值的时候，就会通过输出产生一个输出信号。

## 2 神经元激活函数
激活函数可以看作滤波器，接受外界各种信号，通过调整函数，输出期望值。  
ANN通常采用的激活函数分为：阈值函数，分段函数，双极性连续函数。
<img src="/Users/duanshuai/Downloads/hisi/ann_macdown_01.png" style="zoom:70%" />  

## 3 神经网络拓扑结构
常见的拓扑结构有单层前向网络、多层前向网络、反馈网络，随机神经网络、竞争神经网络。
<img src="/Users/duanshuai/Downloads/hisi/ann_macdown_02.png" style="zoom:70.66%" />  
<img src="/Users/duanshuai/Downloads/hisi/ann_macdown_03.png" style="zoom:69%" />  
## 4 神经网络选择权值

## 5 学习算法
神经网络的学习也被称之为训练，通过神经网络所在环境的刺激作用调整神经网络的自由参数，使神经网络以一种新的方式对外部环境作出反应的一个过程。  
每个神经网络都有一个激活函数：y=f(x)，训练的过程就是通过给定的海量x数据和y数据，拟合出激活函数f。学习过程分为有导师学习和无导师学习。
  
- 有导师学习  
给定期望输出，通过对权值的调整使实际输出逼近期望输出  
- 无导师学习  
给定表示方法质量的测量尺度，根据该尺度来优化参数，常见的有hebb学习，纠错学习，基于记忆学习，随机学习，竞争学习。  

	1. hebb学习  
  		最早提出的学习方法，原理是如果突出两边两个神经元同时被激活，则该突触的能量(权重)就选择性增加；如果被异步激活，则该突触能量减弱或消除。
  	2. 纠错学习  
  		计算实际输出和期望输出的误差，再返回误差，修改权值。原理简单，用到最多，最小梯度下降法（LMS最小均方误差算法）就是这种方法。  
  	3. 基于记忆的学习  
  		主要用于模式分类，在基于记忆的学习中，过去的学习结果被存储在一个大的存储器中，当输入一个新的测试向量时，学习过程就是把新向量归到已存储的某个类中。算法包括两部分：一是用于定义测试向量局部领域的标准；二是在局部领域训练样本的学习规则。常用最近邻规则。
	4. 随机学习算法  
		也叫Bolzmann学习规则，根据最大似然规则，通过调整权值，最小化似然函数或其对数。
模拟退火算法是从物理和化学退火过程类推过来，是“对物体加温后再冷却的处理过程”的数学建模。整个过程分为两步：首先在高温下进行搜索，此时各状态出现概率相差不大，可以很快进入“热平衡状态”，这时进行的是“粗搜索”，也就是大致找到系统的低能区区域；随着温度降低，各状态出现的概率差距逐渐被扩大，搜索精度不断提高，这就可以越来越准确地找到网络能量函数的全局最小点。
	5. 竞争学习  
		神经网络的输出神经元之间相互竞争，在任一时间只能有一个输出神经元是活性的。
		
## 6 神经网络的发展

- 单层感知器
 <img src="/Users/duanshuai/Downloads/hisi/ann_macdown_6_1.jpg" style="zoom:70%" />    
 1958年提出，与MP模型不同处在于权值可变，这样就可以进行学习。它包含一个线性累加器和二值阈值元件（激活函数是阈值函数），还包括外部偏差b。单层感知器被设计用来对输入进行二分类，当感知器输出+1时，输入为一类；当输出为-1时，输入为另一类。之后还有应用LMS算法的单层感知器。  
 <img src="/Users/duanshuai/Downloads/hisi/ann_macdown_6_2.jpg" style="zoom:70%" />    
 单层感知器的缺陷是只能对线性问题分类。如下图，左边能用一根线分开，但右边却不能。  
  <img src="/Users/duanshuai/Downloads/hisi/ann_macdown_6_3.png" style="zoom:90%" />    
 该缺陷来自激活函数。改进思路就是修改激活函数（把分类线变成曲线，如椭圆线）、增加神经网络层数（让两条直线或多条直线来分类）。主流做法是增加层数，于是有了多层感知器。

- 多层感知器
 在输入层和输出层之间增加隐含层（因为不能在训练样本中观察到它们的值，所以叫隐含层）。  
 <img src="/Users/duanshuai/Downloads/hisi/ann_macdown_6_4.png" style="zoom:80%" />    
 多层感知器分类能力如下：    
  <img src="/Users/duanshuai/Downloads/hisi/ann_macdown_6_5.png" style="zoom:80%" />      
  随着隐层层数的增多，凸域将可以形成任意的形状，因此可以解决任何复杂的分类问题。Kolmogorov理论指出：双隐层感知器就足以解决任何复杂的分类问题。但层数的增多带来隐含层的权值训练问题，对于各隐层的节点来说，它们并不存在期望输出，所以也无法通过感知器的学习规则来训练多层感知器。1966年，Minisky和Papert在他们的《感知器》一书中提出了上述的感知器的研究瓶颈，指出理论上还不能证明将感知器模型扩展到多层网络是有意义的。人工神经网络进入低谷期。直到出现误差反向传播算法（BP：ErrorBack Propagation），解决了多层感知器的学习问题。
 
  
 #BACKPROP
 前馈神经网络，是神经网络的一种，也是最常用的一种。它包括一个输入层，一个输出层，和若干隐含层。因此，具有改种拓扑结构的神经网络又被称为多层感知器(MLP
).  
 
